tasks: ['pick_up_cup'] # ['pick_up_cup', 'pick_and_lift', 'push_button', 'reach_target', 'take_lid_off_saucepan']
10_tasks:  [pick_up_cup,phone_on_base,pick_and_lift,put_rubbish_in_bin,push_button,reach_target,lamp_on,take_lid_off_saucepan,take_umbrella_out_of_umbrella_stand,stack_wine]
run_name: burn 
mt_only: False # True for no context, MT policy 
log_path: ??? 
tasks_name: ???
rlbench: 
    tasks: ${tasks}
    all_tasks: ???       # [task_A, task_B,...]
    use_variations: ??? # [ [task_A_0], [task_B_0], ...
    all_variations: ???  # [ [task_A_0, task_A_1,...], [task_B_0, ...] ]
    id_to_tasks: ??? 
    demos: 3
    demo_path: /home/mandi/all_rlbench_data
    episode_length: 10
    cameras: [front]
    camera_resolution: [128, 128]
    scene_bounds: [-0.3, -0.5, 0.6, 0.7, 0.5, 1.6]
    max_fails: 5
    single_variation: False 
    num_vars: -1 

replay:
    batch_size: 64 # total buff size, no task-aggregate anymore 
    total_batch_size: 10000
    timesteps: 1
    prioritisation: True
    use_disk: False
    path: '/tmp/arm_sanity/replay'  # Only used when use_disk is True.
    share_across_tasks: False  
    replay_size: 100000 # total size 
    update_buffer_prio: True 
    buffers_per_batch: 20

framework:
    log_freq:  100 # set to bigger to not save so many media files 
    save_freq: 100
    train_envs: 1
    eval_envs: 1
    env_runner_gpu: 2
    replay_ratio: 3
    transitions_before_train: 100
    tensorboard_logging: True
    csv_logging: False
    wandb_logging: True
    training_iterations: 30000
    gpu: 0
    logdir: '/home/mandi/ARM/log/'
    seeds: 1
    resume: ${resume}
    resume_path: ${resume_path}
    resume_run: ${resume_run}
    resume_step:  ${resume_step}
    resume_dir: ''
    log_all_vars: False
    switch_online_tasks: 0


resume: False
resume_path: '/home/mandi/ARM/log/'
resume_run: ''
resume_step: 24300

dev:
    q_thres: 0.75  
    qagent_update_context: True
    qagent_use_emb_loss: True # for ablation set to False 
    one_hot: False 
    encode_context: False  # if one-hot, prolly ok not to encode
    qnet_context_latent_size: 16
    cat_down1: False
    cat_down2: False
    cat_up1: False 
    cat_up2: False
    cat_f1: False
    conv3d: False 
    handpick: [] # [0,3,4,6,7,10,11,16,18,19] or, [0,3,4,6,10] for red, green,blue,yellow,gray
    # exclude similar colors: red, green,blue,yellow,cyan,gray,orange,violet,black,white
    # exclude gray and white: [0,3,4,6,7,11,16,18]
    offline: False 
    eval_only: False 

contexts:
  num_update_itrs: 5
  val_freq:   100
  update_freq: 100000  # as compared to agent update
  pass_down_context: True 
  pretrain_context_steps: 0 
  agent:
    with_action_context: False
    is_train: True
    embedding_size: 16
    query_ratio: 0.3 # takes this much out of the total K samples 
    margin:     0.1
    emb_lambda: 1.0
    save_context: False 
    loss_mode:  'hinge' 
    prod_of_gaus_factors_over_batch: False # for PEARL 
    encoder_cfg: ${encoder}
    replay_update: False 
    single_embedding_replay: True # if False, draw equal amount of embeddings from context 
  sampler:
    batch_dim:             ${replay.buffers_per_batch}   # dimension 'B' -> this has to match num of buffers
    k_dim:                 ${replay.batch_size}      # dimension 'K'     -> this doesnt 
    drop_last:             True 
    sample_mode:           'variation' # or 'variation'
    val_batch_dim:             10      # actually let's fix this for all sweeping runs to get a fair comparision 
    val_k_dim: 2                       # dimension 'K', has to limit to 2 due to datasplit

dataset:
  image_size:                 [128, 128]
  root_dir:                   ${rlbench.demo_path}
  num_variations_per_task:    -1 # just loads all vars
  num_episodes_per_variation: 20 # up to 20
  num_steps_per_episode:      2 # 1, 2 for now  
  exclude_tasks:              []
  include_tasks:              ${tasks} # OR: set to [] or other options so it loads in more tasks than the trained one
  data_augs:   
    grayscale:        0
    strong_jitter:    0 
    weak_crop_scale:  [0.7, 0.9]
    weak_crop_ratio:  [0.9, 1]
  split:              [0.9, 0.1] # train/val split

defaults:
    - method: C2FARM
    - encoder: SlowRes

wandb:
    project: 'ContextARM'
    job_type: 'launch'
    group:    'meta_context'
hydra:
    run:
        dir: ${framework.logdir}/ #${rlbench.task}/${method.name}
