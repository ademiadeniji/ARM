tasks: ['pick_up_cup']
run_name: burn 
mt_only: False # True for no context, MT policy 
log_path: ??? 
tasks_name: ???
rlbench: 
    tasks: ${tasks}
    all_tasks: ???      # [task_A, task_B,...]
    all_variations: ??? # [ [task_A_0, task_A_1,...], [task_B_0, ...] ]
    id_to_tasks: ???
    # variations: -1         # Actually it's useless to limit this, 
    demos: 3
    demo_path: /home/mandi/all_rlbench_data
    episode_length: 10
    cameras: [front]
    camera_resolution: [128, 128]
    scene_bounds: [-0.3, -0.5, 0.6, 0.7, 0.5, 1.6]
    max_fails: 5

replay:
    batch_size: 64 # total buff size, no task-aggregate anymore 
    total_batch_size: ???
    timesteps: 1
    prioritisation: True
    use_disk: False
    path: '/tmp/arm_sanity/replay'  # Only used when use_disk is True.
    share_across_tasks: True
    buffer_key: 'variation_id' # either var or task id, use this to look for buffers
    replay_size: 200000 # total size 

framework:
    log_freq:  100 # set to bigger to not save so many media files 
    save_freq: 100
    train_envs: 3
    eval_envs: 3
    env_runner_gpu: 2
    replay_ratio: ${replay.batch_size}
    transitions_before_train: 100
    tensorboard_logging: True
    csv_logging: False
    wandb_logging: True
    training_iterations: 10000
    gpu: 0
    logdir: '/home/mandi/ARM/log/'
    seeds: 1
    resume: ${resume}
    resume_path: ${resume_path}
    resume_run: ${resume_run}
    resume_step:  ${resume_step}
    resume_dir: ''


resume: False
resume_path: '/home/mandi/ARM/log/'
resume_run: ''
resume_step: 24300

dev:
    q_thres: 0.75  
    qagent_update_context: True
    one_hot: False
    one_hot_size: 20   
    encode_context: False  # if one-hot, prolly ok not to encode
    qnet_context_latent_size: 16
    cat_down1: 1
    cat_down2: 1
    cat_up1: 1
    cat_up2: 1
    cat_f1: 1
    buffers_per_batch: 20

contexts:
  num_update_itrs: 5
  val_freq:   100
  update_freq: 2  # as compared to agent update
  pass_down_context: True 
  pretrain_context_steps: 0 
  agent:
    with_action_context: False
    is_train: True
    embedding_size: 32 
    num_support: 2  #for TecNet
    num_query:  1
    margin:     2.0
    emb_lambda: 1.0
    save_context: False 
    loss_mode:  'hinge' 
    prod_of_gaus_factors_over_batch: False # for PEARL 
    encoder_cfg: ${encoder}
  sampler:
    batch_dim:             10    # dimension 'B'
    samples_per_variation: 3      # dimension 'N' 
    drop_last:             True 
    sample_mode:           'variation' # or 'variation'
    val_batch_dim:             20      # actually let's fix this for all sweeping runs to get a fair comparision 
    val_samples_per_variation: 2      # dimension 'N', has to limit to 2 due to datasplit

dataset:
  image_size:                 [128, 128]
  root_dir:                   ${rlbench.demo_path}
  num_variations_per_task:    -1 # just loads all vars, no way to constraint envrunner anyway
  num_episodes_per_variation: 20 # up to 20
  num_steps_per_episode:      2 # 1, 2 for now  
  exclude_tasks:              []
  include_tasks:              ${tasks} # OR: set to [] or other options so it loads in more tasks than the trained one
  data_augs:   
    grayscale:        0
    strong_jitter:    0 
    weak_crop_scale:  [0.7, 0.9]
    weak_crop_ratio:  [0.9, 1]
  split:              [0.9, 0.1] # train/val split

defaults:
    - method: C2FARM
    - encoder: SlowRes

wandb:
    project: 'ContextARM'
    job_type: 'launch'
    group:    'meta_context'
hydra:
    run:
        dir: ${framework.logdir}/ #${rlbench.task}/${method.name}
